ã€Œãƒãƒ¼ãƒˆã‚’æ”¹é€ ã—ã¦OOFã‚’å–ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€ãŸã‚ã® **æœ€ä½é™ã®ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³é›†** ã‚’æ•´ç†ã—ã¾ã™ã€‚
åŸºæœ¬çš„ã«ã¯ã€Œã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ã©ã“ã« `oof_preds[val_idx] = ...` ã‚’å…¥ã‚Œã‚‹ã‹ï¼Ÿã€ãŒéµã§ã™ã€‚

---

# ğŸ”‘ OOFæ”¹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³é›†

## â‘  KFold / StratifiedKFold ã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆ

ã‚ˆãã‚ã‚‹å…¸å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

```python
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import numpy as np

kf = KFold(n_splits=5, shuffle=True, random_state=42)
oof_preds = np.zeros(len(train_x))

for fold, (train_idx, val_idx) in enumerate(kf.split(train_x, train_y)):
    model = lgb.LGBMRegressor()
    model.fit(train_x.iloc[train_idx], train_y.iloc[train_idx])
    val_pred = model.predict(train_x.iloc[val_idx])
    
    oof_preds[val_idx] = val_pred
    print(f"Fold {fold+1} RMSE:", mean_squared_error(train_y.iloc[val_idx], val_pred, squared=False))

np.save("oof.npy", oof_preds)
```

### ãƒã‚¤ãƒ³ãƒˆ

* `val_idx` ã« fold ã”ã¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå…¥ã£ã¦ã„ã‚‹
* `oof_preds[val_idx] = val_pred` ãŒ **å¿…é ˆè¡Œ**

---

## â‘¡ cross\_val\_score / cross\_val\_predict ã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆ

Scikit-learnæµã«ç°¡ç•¥åŒ–ã—ã¦æ›¸ã„ã¦ã„ã‚‹ãƒãƒ¼ãƒˆã«é­é‡ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

```python
from sklearn.model_selection import cross_val_predict
from sklearn.linear_model import Ridge

model = Ridge()
oof_preds = cross_val_predict(model, train_x, train_y, cv=5)
np.save("oof.npy", oof_preds)
```

### ãƒã‚¤ãƒ³ãƒˆ

* `cross_val_predict` ã‚’ä½¿ãˆã° **ä¸€ç™ºã§OOFãŒå–ã‚Œã‚‹**
* ãŸã ã— LightGBM ã‚„ XGBoost ã§ã¯ä½¿ãˆãªã„ã“ã¨ãŒå¤šã„ï¼ˆscikit-learn APIã§åŒ…ã‚“ã§ã„ãªã„ã¨ç„¡ç†ï¼‰

---

## â‘¢ train\_test\_split ã‚’æ‰‹ä½œæ¥­ã§ãƒ«ãƒ¼ãƒ—ã—ã¦ã„ã‚‹å ´åˆ

ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§ãªãã€Œæ‰‹æ›¸ãã§åˆ†å‰²ã€ã—ã¦ã„ã‚‹ãƒãƒ¼ãƒˆã€‚
ã“ã®å ´åˆã‚‚è€ƒãˆæ–¹ã¯åŒã˜ã§ã™ã€‚

```python
from sklearn.model_selection import train_test_split

oof_preds = np.zeros(len(train_x))

for seed in [0, 1, 2, 3, 4]:
    X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(
        train_x, train_y, np.arange(len(train_x)), test_size=0.2, random_state=seed
    )
    model = lgb.LGBMRegressor()
    model.fit(X_train, y_train)
    val_pred = model.predict(X_val)
    
    oof_preds[idx_val] = val_pred
```

### ãƒã‚¤ãƒ³ãƒˆ

* `train_test_split` ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã•ãªã„ã®ã§ã€è‡ªåˆ†ã§ä½œã‚‹å¿…è¦ãŒã‚ã‚‹
  â†’ `np.arange(len(train_x))` ã‚’ä¸€ç·’ã«splitã—ã¦å¯¾å¿œ

---

## â‘£ TimeSeriesSplitï¼ˆæ™‚ç³»åˆ—CVï¼‰ã®å ´åˆ

Kaggleã§ã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã‚ˆãç™»å ´ã—ã¾ã™ã€‚

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
oof_preds = np.zeros(len(train_x))

for fold, (train_idx, val_idx) in enumerate(tscv.split(train_x)):
    model = lgb.LGBMRegressor()
    model.fit(train_x.iloc[train_idx], train_y.iloc[train_idx])
    val_pred = model.predict(train_x.iloc[val_idx])
    
    oof_preds[val_idx] = val_pred
```

### ãƒã‚¤ãƒ³ãƒˆ

* `shuffle=True` ãŒä½¿ãˆãªã„
* æœªæ¥æƒ…å ±ã‚’æ¼ã‚‰ã•ãªã„ã‚ˆã†ã«ã€Œtrain < valã€ã¨ã„ã†é †åºãŒä¿è¨¼ã•ã‚Œã¦ã„ã‚‹

---

# âœ… ã¾ã¨ã‚

* **å¿…ãšã‚ã‚‹ã¹ãè¡Œ** â†’ `oof_preds[val_idx] = model.predict(X_val)`
* ã©ã®CVæ‰‹æ³•ã§ã‚‚ã€**æ¤œè¨¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«äºˆæ¸¬ã‚’åŸ‹ã‚è¾¼ã‚€**ã®ãŒå…±é€šåŸå‰‡
* æœ€å¾Œã« `np.save("oof.npy", oof_preds)` ã—ã¦åé›†ã§ãã‚‹å½¢ã«ã™ã‚‹

---


